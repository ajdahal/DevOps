
Create a gitlab account

Gitlab over Github: CI pipeline and docker support | more features for DEVOPS 

Go to you account ( top right ) >> Preferences >> click on "Render Whitespace characters in Web IDE" button     //It will be very helpful in the later stage 
The .gitlab-ci.yml is very important as it allows us the build our pipeline. It can have content such as the one below -

test:
    script: echo "Hello World"

Commit the changes and Go to the project to see if the pipeline failed/succeeded. 
See the Jobs section to see how the Job executed ... By default ruby docker image is downloaded and job runs on that docker iamge 
We can specify the docker image on which we want to run our Job.

.gitlab-ci.yml

build laptop:
    image: alpine 
    script: 
        - echo "Building a laptop"
        - mkdir build
        - touch build/computer.txt
        - echo "Mainboard" >> build/computer.txt
        - cat build/computer.txt
        - echo "Keyboard" >> build/computer.txt
        - cat build/computer.txt

image cannot be images, script cannot be scripts - It will still be valid yaml but Gitlab willn't be understand images, scripts.

Validate Indentation and Correct .yml is in place.

Yaml follows key-value pair principle, is used widely in DevOps.
Indentation is used for scope in Yaml.

Person:
    name: Arjun 
    age: 25 
    hobbies:
        - Programming
        - YouTube
        - hiking
    address:
        street: Siddiganesh 2.
        city: Madhyapur Thimi
        District: Bhaktapur
    experience:
        - title: Associate Solutions Engineer
          period: 2018 - 2020
        - title:  Solutions Engineer
          period: Since 2020

Array vs Dictionary, Array of Dictionaries 
validate the yaml using online yaml validator or in GitLab-IDE as well.


Gitlab architecture:

Gitlab Server (Manages the pipelines and the associated jobs, stores the results)  ---->  Gitlab Runner (Exectues the Job, Can be multiple in number)
GitLab Runner --> Retrieve set of instructions from Gitlab Server, Download and start docker image specified, Download code from project repository, run all the commands specified in the job and report back the result of execution to Gitlab Server. When the job completes, the docker container will be destroyed. 

Every job runs in a docker container - flexibility, security | Job specifies the scripts or tasks to perform | Gitlab runner can be installed and used on a laptop. If a single stage fails in pipeline, the whole pipeline fails. You can define stages in Yaml file.

Job Artifacts:
Artifact is Job output that we want to save and use in another stage. The issue with test laptop is because the alpine container used in build laptop isn't used iin test laptop state. We will be using artifact to save necessary changes.


stages:
    - build
    - test 

build laptop:
    image: alpine
    stage: build
    script: 
        - echo "Building a laptop"
        - mkdir build
        - touch build/computer.txt
        - echo "Mainboard" >> build/computer.txt
        - cat build/computer.txt
        - echo "Keyboard" >> build/computer.txt
        - cat build/computer.txt
    artifacts:
        paths: 
            - build

test laptop:
    image: alpine
    stage: test
    script:
        - test -f build/computer.txt


The Gitlab Runner gives back the files inside build folder to Gitlab server after artifacts is mentioned. When the 2nd Gitlab Runner runs for "test laptop" stage the folder "build" is downloaded on new alpine container and the commands inside this state run aftwerwards.
Local variables can be defined inside scripts as: build_file_name=laptop.txt. These variables can be defined globally as: BUILD_FILE_NAME: "laptop.txt".
The variables can be used inside the stages as $BUILD_FILE_NAME.

DevOps isnot a standard or specification, a tool or a particular software, something you do with a tool or software.
DevOps is cultural thing, change in mindset, Automating Tasks, automatically building and deploying software (CI/CD).

The image "alpine" doesn't work as it doesn't have all dependencies, the image "node" takes took long time as it's heavyweight image ( ~100 MB's) in size resulting in higher duration of Jobs. The image that would be suitable for this case is 16-alpine which is relatively small ( nearly 40MB in size ) and has all the dependencies.

stages:
    - Build
    - Available
    - Test

variables:
    INDEX_FILE_LOCATION: "index.html"

build website:
    image: node:16-alpine
    stage: Build

    script:
        - yarn install
        - yarn build
    
    artifacts:
        paths:
            - build

File Index Available:
    image: node:16-alpine
    stage: Available
    script: 
        - test -f build/$INDEX_FILE_LOCATION

Unit Test:
    image: node:16-alpine
    stage: Test
    script:
        - yarn install
        - yarn test

Don't push changes directly to the main branch, work with Git Branches which would later be integrated in main branch.
If pipeline works on feature branch, merge it with Main branch else don't merge, fix the merge issues, or discard the branch if you don't want to work with it.

-- made some tweaks in the settings related to merge -- 

Two jobs can have same stage .pre ( this is predefined stage, need not explicitely define it under stages section )
The jobs that don't have dependency on build can be assigned .pre stage. Here, linter and Unit Test are assigned stage .pre. 

The new branch name should be descriptive like "feature/added linter to the project". 
Don't allow the option to merge the code of branch directly to main. Instead set option for someone else (other than the developer) to review the code changes and then only allow or reject the merge request.


stages:
    - Build
    - Available
    - Test

variables:
    INDEX_FILE_LOCATION: "index.html"

build website:
    image: node:16-alpine
    stage: Build

    script:
        - yarn install
        - yarn build
    
    artifacts:
        paths:
            - build

.linter:
    image: node:16-alpine
    stage: .pre 
    script:
        - yarn install
        - yarn lint


File Index Available:
    image: node:16-alpine
    stage: Available
    script: 
        - yarn global add serve
        - serve -s build 

.Unit Test:
    image: node:16-alpine
    stage: .pre
    script:
        - yarn install
        - yarn test


To disable the job, you would have to put . in front of the Job like here - .linter, .Unit Test.

"    - serve -s build " is the integration test. Since, the serve command isn't found in node:16-alpine image, we have added "yarn global add serve" command to use this command.

The issue with above yaml configuration is that a server spuns up at localhost port 3000, and runs continuously till it timesout (TimeOut param set to 60 minute).
The output in the terminal during the build process will be as follows - 

$ serve -s build
INFO: Accepting connections at http://localhost:3000


However, we can make changes to the .yaml config to run the server in background and when curl command has been executed server ends. Curl command isn't present on the node:16-alpine image, thus we are using apk (package manager of alpine) to get curl command. The server takes some time ( few seconds ) to start, thus we are waiting till the server starts and then running curl command.
......
File Index Available:
    image: node:16-alpine
    stage: Available
    script: 
        - yarn global add serve
        - apk add curl
        - serve -s build &
        - sleep 10 
        - curl http://localhost:3000 | grep "React App"          //"React App" is present in Page source 
......

Order of Integration Tests ??
There is no hard rule, but there are some general ideas.

1. Failing fast 
Generally, grouping linting, unit tests and running them first in pipeline is better for fast feedback.
Jobs of similar size should be grouped together. Jobs that run in parallel should take nearly same time for the test.

2. Dependencies between the Job
linter and unit tests don't require build output so can be run at first. However, we need to test website after the website is build.

The jobs can be reduced from above to as below to save time - 

stages:
    - Build
    - Test

build website:
    image: node:16-alpine
    stage: Build

    script:
        - yarn install
        - yarn lint
        - yarn test
        - yarn build
    artifacts:
        paths:
            - build

test website:
    image: node:16-alpine
    stage: Test
    script: 
        - yarn global add serve
        - apk add curl
        - serve -s build &
        - sleep 10 
        - curl http://localhost:3000 | grep "React App"


AWS:
Now, we are trying to move our website to s3 and work with AWS on our website deployment.
Variables can be declared in Settings >> CI/CD >> Variables >> Add Variables
Protect Variable --> Currently, our main branch is protected. If this option is enabled, only main branch will access the variable we have saved in this section.
Thus, a developer using feature branch cannot access the variable and deploy his/her code to the production immediately after making a code change.
Enable Mask Variable as variables ( Eg: passwords ) can be sensitive. 

A bucket was created with programmatic access in AWS, Access Key, Secret Key, Bucket Name, AWS Region were configured as variables in BitBucket after which the Job was triggred again.
It resulted in successful upload of the file to s3 bucket specified.

Go to properties of S3 >> Host static Website >> Enable public access in permissions  >> Edit Bucket Policy to allow public access to resources inside the bucket

Gitlab has rules that will allow to run a job if it's present on main branch but doesn't allow it to run on a branch.
How do we check if the website still works after the deployment? Add a post-deployment stage in the pipeline. It would be addition in configuration as below - 

variables:
    APP_BASE_URL: http://devops-fundamentals.s3-website-us-east-1.amazonaws.com

production tests:
    image: curlimages/curl
    stage: post deploy
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script: 
    - curl $APP_BASE_URL | grep "React App"


Generally, we have staging environment where we check things before production environment.
Staging Environment: Non-public environment which is very similar to Production Environment.

Continuous Integration ( Build, Test ) -- Continuous Deployment ( Staging, Production )   # Every commit goes to Production 
Continuous Integration ( Build, Test ) -- Continuous Delivery ( Staging -- PUSH A BUTTON -- Production ) #You need to manually select the option to push the changes to production

Serially, we would have the following stages in deployment:

Build
Test
Deploy Staging
Test Staging
--- If something breaks at "Test Staging", the changes made wouldn't be Deployed to production Environment ---
Deploy Production
Test Production


Production Deployment and test states are as follows. The staging stage will differ from production in this case with regards to s3 bucket name.

deploy to production:
    stage: deploy
    image:
        name: amazon/aws-cli:2.5.6
        entrypoint: [""]
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script:
        - aws --version
        - echo $CI_COMMIT_REF_NAME
        - aws s3 sync build s3://$AWS_S3_BUCKET --delete

production tests:
    image: curlimages/curl
    stage: post deploy
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script: 
    - curl $APP_BASE_URL | grep "React App"

Managing the Environment: 
Deployments >> Environment >> Configure staging and production buckets urls.
Staging is an Environment, Production is an Environment

AWS_S3_BUCKET variable can be used as a name for both production and staging environment under variable section. Making changes to the .yml file to adapt to
to this new convention - 

stages:
    .....
    - staging deploy
    - production deploy
    .....

.....

deploy to staging:
    stage: staging deploy
    environment: staging
    image:
        name: amazon/aws-cli:2.5.6
        entrypoint: [""]
    script:
        - aws --version
        - echo $CI_COMMIT_REF_NAME
        - aws s3 sync build s3://$AWS_S3_BUCKET --delete
        - curl $CI_ENVIRONMENT_URL | grep "React App"

deploy to production:
    stage: production deploy
    environment: production
    image:
        name: amazon/aws-cli:2.5.6
        entrypoint: [""]
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script:
        - aws --version
        - echo $CI_COMMIT_REF_NAME
        - aws s3 sync build s3://$AWS_S3_BUCKET --delete
        - curl $CI_ENVIRONMENT_URL | grep "React App"

Reusing Configuration:

stages:
    - build
    - test
    - staging deploy
    - production deploy

variables:
    APP_VERSION: $CI_PIPELINE_IID

build website:
    image: node:16-alpine
    stage: build

    script:
        - yarn install
        - yarn lint
        - yarn test
        - yarn build
        - echo $APP_VERSION > build/version.html            # Version of build is sent to version.html
    artifacts:
        paths:
            - build

......

.deploy:
    image:
        name: amazon/aws-cli:2.5.6
        entrypoint: [""]
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script:
        - aws --version
        - aws s3 sync build s3://$AWS_S3_BUCKET --delete
        - curl $CI_ENVIRONMENT_URL | grep "React App"
        - curl $CI_ENVIRONMENT_URL/version.html | grep $APP_VERSION

deploy to staging:
    stage: staging deploy
    environment: staging
    extends: .deploy

deploy to production:
    stage: production deploy
    when: manual                                                    # This results in manual intervention before the changes are deployed to production 
    environment: production
    extends: .deploy

Elastic Beanstock:
Platform: Docker