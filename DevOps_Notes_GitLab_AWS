
Create a gitlab account

Gitlab over Github: CI pipeline and docker support | more features for DEVOPS 

Go to you account ( top right ) >> Preferences >> click on "Render Whitespace characters in Web IDE" button     //It will be very helpful in the later stage 
The .gitlab-ci.yml is very important as it allows us the build our pipeline. It can have content such as the one below -

test:
    script: echo "Hello World"

Commit the changes and Go to the project to see if the pipeline failed/succeeded. 
See the Jobs section to see how the Job executed ... By default ruby docker image is downloaded and job runs on that docker iamge 
We can specify the docker image on which we want to run our Job.

.gitlab-ci.yml

build laptop:
    image: alpine 
    script: 
        - echo "Building a laptop"
        - mkdir build
        - touch build/computer.txt
        - echo "Mainboard" >> build/computer.txt
        - cat build/computer.txt
        - echo "Keyboard" >> build/computer.txt
        - cat build/computer.txt

image cannot be images, script cannot be scripts - It will still be valid yaml but Gitlab willn't be understand images, scripts.

Validate Indentation and Correct .yml is in place.

Yaml follows key-value pair principle, is used widely in DevOps.
Indentation is used for scope in Yaml.

Person:
    name: Arjun 
    age: 25 
    hobbies:
        - Programming
        - YouTube
        - hiking
    address:
        street: Siddiganesh 2.
        city: Madhyapur Thimi
        District: Bhaktapur
    experience:
        - title: Associate Solutions Engineer
          period: 2018 - 2020
        - title:  Solutions Engineer
          period: Since 2020

Array vs Dictionary, Array of Dictionaries 
validate the yaml using online yaml validator or in GitLab-IDE as well.


Gitlab architecture:

Gitlab Server (Manages the pipelines and the associated jobs, stores the results)  ---->  Gitlab Runner (Exectues the Job, Can be multiple in number)
GitLab Runner --> Retrieve set of instructions from Gitlab Server, Download and start docker image specified, Download code from project repository, run all the commands specified in the job and report back the result of execution to Gitlab Server. When the job completes, the docker container will be destroyed. 

Every job runs in a docker container - flexibility, security | Job specifies the scripts or tasks to perform | Gitlab runner can be installed and used on a laptop. If a single stage fails in pipeline, the whole pipeline fails. You can define stages in Yaml file.

Job Artifacts:
Artifact is Job output that we want to save and use in another stage. The issue with test laptop is because the alpine container used in build laptop isn't used iin test laptop state. We will be using artifact to save necessary changes.


stages:
    - build
    - test 

build laptop:
    image: alpine
    stage: build
    script: 
        - echo "Building a laptop"
        - mkdir build
        - touch build/computer.txt
        - echo "Mainboard" >> build/computer.txt
        - cat build/computer.txt
        - echo "Keyboard" >> build/computer.txt
        - cat build/computer.txt
    artifacts:
        paths: 
            - build

test laptop:
    image: alpine
    stage: test
    script:
        - test -f build/computer.txt


The Gitlab Runner gives back the files inside build folder to Gitlab server after artifacts is mentioned. When the 2nd Gitlab Runner runs for "test laptop" stage the folder "build" is downloaded on new alpine container and the commands inside this state run aftwerwards.
Local variables can be defined inside scripts as: build_file_name=laptop.txt. These variables can be defined globally as: BUILD_FILE_NAME: "laptop.txt".
The variables can be used inside the stages as $BUILD_FILE_NAME.

DevOps isnot a standard or specification, a tool or a particular software, something you do with a tool or software.
DevOps is cultural thing, change in mindset, Automating Tasks, automatically building and deploying software (CI/CD).

The image "alpine" doesn't work as it doesn't have all dependencies, the image "node" takes took long time as it's heavyweight image ( ~100 MB's) in size resulting in higher duration of Jobs. The image that would be suitable for this case is 16-alpine which is relatively small ( nearly 40MB in size ) and has all the dependencies.

stages:
    - Build
    - Available
    - Test

variables:
    INDEX_FILE_LOCATION: "index.html"

build website:
    image: node:16-alpine
    stage: Build

    script:
        - yarn install
        - yarn build
    
    artifacts:
        paths:
            - build

File Index Available:
    image: node:16-alpine
    stage: Available
    script: 
        - test -f build/$INDEX_FILE_LOCATION

Unit Test:
    image: node:16-alpine
    stage: Test
    script:
        - yarn install
        - yarn test

Don't push changes directly to the main branch, work with Git Branches which would later be integrated in main branch.
If pipeline works on feature branch, merge it with Main branch else don't merge, fix the merge issues, or discard the branch if you don't want to work with it.

-- made some tweaks in the settings related to merge -- 

Two jobs can have same stage .pre ( this is predefined stage, need not explicitely define it under stages section )
The jobs that don't have dependency on build can be assigned .pre stage. Here, linter and Unit Test are assigned stage .pre. 

The new branch name should be descriptive like "feature/added linter to the project". 
Don't allow the option to merge the code of branch directly to main. Instead set option for someone else (other than the developer) to review the code changes and then only allow or reject the merge request.


stages:
    - Build
    - Available
    - Test

variables:
    INDEX_FILE_LOCATION: "index.html"

build website:
    image: node:16-alpine
    stage: Build

    script:
        - yarn install
        - yarn build
    
    artifacts:
        paths:
            - build

.linter:
    image: node:16-alpine
    stage: .pre 
    script:
        - yarn install
        - yarn lint


File Index Available:
    image: node:16-alpine
    stage: Available
    script: 
        - yarn global add serve
        - serve -s build 

.Unit Test:
    image: node:16-alpine
    stage: .pre
    script:
        - yarn install
        - yarn test


To disable the job, you would have to put . in front of the Job like here - .linter, .Unit Test.

"    - serve -s build " is the integration test. Since, the serve command isn't found in node:16-alpine image, we have added "yarn global add serve" command to use this command.

The issue with above yaml configuration is that a server spuns up at localhost port 3000, and runs continuously till it timesout (TimeOut param set to 60 minute).
The output in the terminal during the build process will be as follows - 

$ serve -s build
INFO: Accepting connections at http://localhost:3000


However, we can make changes to the .yaml config to run the server in background and when curl command has been executed server ends. Curl command isn't present on the node:16-alpine image, thus we are using apk (package manager of alpine) to get curl command. The server takes some time ( few seconds ) to start, thus we are waiting till the server starts and then running curl command.
......
File Index Available:
    image: node:16-alpine
    stage: Available
    script: 
        - yarn global add serve
        - apk add curl
        - serve -s build &
        - sleep 10 
        - curl http://localhost:3000 | grep "React App"          //"React App" is present in Page source 
......

Order of Integration Tests ??
There is no hard rule, but there are some general ideas.

1. Failing fast 
Generally, grouping linting, unit tests and running them first in pipeline is better for fast feedback.
Jobs of similar size should be grouped together. Jobs that run in parallel should take nearly same time for the test.

2. Dependencies between the Job
linter and unit tests don't require build output so can be run at first. However, we need to test website after the website is build.

The jobs can be reduced from above to as below to save time - 

stages:
    - Build
    - Test

build website:
    image: node:16-alpine
    stage: Build

    script:
        - yarn install
        - yarn lint
        - yarn test
        - yarn build
    artifacts:
        paths:
            - build

test website:
    image: node:16-alpine
    stage: Test
    script: 
        - yarn global add serve
        - apk add curl
        - serve -s build &
        - sleep 10 
        - curl http://localhost:3000 | grep "React App"


AWS:
Now, we are trying to move our website to s3 and work with AWS on our website deployment.
Variables can be declared in Settings >> CI/CD >> Variables >> Add Variables
Protect Variable --> Currently, our main branch is protected. If this option is enabled, only main branch will access the variable we have saved in this section.
Thus, a developer using feature branch cannot access the variable and deploy his/her code to the production immediately after making a code change.
Enable Mask Variable as variables ( Eg: passwords ) can be sensitive. 

A bucket was created with programmatic access in AWS, Access Key, Secret Key, Bucket Name, AWS Region were configured as variables in BitBucket after which the Job was triggred again.
It resulted in successful upload of the file to s3 bucket specified.

Go to properties of S3 >> Host static Website >> Enable public access in permissions  >> Edit Bucket Policy to allow public access to resources inside the bucket

Gitlab has rules that will allow to run a job if it's present on main branch but doesn't allow it to run on a branch.
How do we check if the website still works after the deployment? Add a post-deployment stage in the pipeline. It would be addition in configuration as below - 

variables:
    APP_BASE_URL: http://devops-fundamentals.s3-website-us-east-1.amazonaws.com

production tests:
    image: curlimages/curl
    stage: post deploy
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script: 
    - curl $APP_BASE_URL | grep "React App"


Generally, we have staging environment where we check things before production environment.
Staging Environment: Non-public environment which is very similar to Production Environment.

Continuous Integration ( Build, Test ) -- Continuous Deployment ( Staging, Production )   # Every commit goes to Production 
Continuous Integration ( Build, Test ) -- Continuous Delivery ( Staging -- PUSH A BUTTON -- Production ) #You need to manually select the option to push the changes to production

Serially, we would have the following stages in deployment:

Build
Test
Deploy Staging
Test Staging
--- If something breaks at "Test Staging", the changes made wouldn't be Deployed to production Environment ---
Deploy Production
Test Production


Production Deployment and test states are as follows. The staging stage will differ from production in this case with regards to s3 bucket name.

deploy to production:
    stage: deploy
    image:
        name: amazon/aws-cli:2.5.6
        entrypoint: [""]
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script:
        - aws --version
        - echo $CI_COMMIT_REF_NAME
        - aws s3 sync build s3://$AWS_S3_BUCKET --delete

production tests:
    image: curlimages/curl
    stage: post deploy
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script: 
    - curl $APP_BASE_URL | grep "React App"

Managing the Environment: 
Deployments >> Environment >> Configure staging and production buckets urls.
Staging is an Environment, Production is an Environment

AWS_S3_BUCKET variable can be used as a name for both production and staging environment under variable section. Making changes to the .yml file to adapt to
to this new convention - 

stages:
    .....
    - staging deploy
    - production deploy
    .....

.....

deploy to staging:
    stage: staging deploy
    environment: staging
    image:
        name: amazon/aws-cli:2.5.6
        entrypoint: [""]
    script:
        - aws --version
        - echo $CI_COMMIT_REF_NAME
        - aws s3 sync build s3://$AWS_S3_BUCKET --delete
        - curl $CI_ENVIRONMENT_URL | grep "React App"

deploy to production:
    stage: production deploy
    environment: production
    image:
        name: amazon/aws-cli:2.5.6
        entrypoint: [""]
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script:
        - aws --version
        - echo $CI_COMMIT_REF_NAME
        - aws s3 sync build s3://$AWS_S3_BUCKET --delete
        - curl $CI_ENVIRONMENT_URL | grep "React App"

Reusing Configuration:

stages:
    - build
    - test
    - staging deploy
    - production deploy

variables:
    APP_VERSION: $CI_PIPELINE_IID

build website:
    image: node:16-alpine
    stage: build

    script:
        - yarn install
        - yarn lint
        - yarn test
        - yarn build
        - echo $APP_VERSION > build/version.html            # Version of build is sent to version.html
    artifacts:
        paths:
            - build

......

.deploy:
    image:
        name: amazon/aws-cli:2.5.6
        entrypoint: [""]
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script:
        - aws --version
        - aws s3 sync build s3://$AWS_S3_BUCKET --delete
        - curl $CI_ENVIRONMENT_URL | grep "React App"
        - curl $CI_ENVIRONMENT_URL/version.html | grep $APP_VERSION

deploy to staging:
    stage: staging deploy
    environment: staging
    extends: .deploy

deploy to production:
    stage: production deploy
    when: manual                                                    # This results in manual intervention before the changes are deployed to production 
    environment: production
    extends: .deploy

Elastic Beanstock:
Platform: Docker

Dockerrun.aws.public.json --> Manifest file that specifies which container to run along with some properties of container.

In GitLab, in order to build docker images, we need our docker daemon to be up and running and our docker client ( made from nginx image ) to be built after 
adding necessary code
dind -> Docker in Docker images 


The docker image that we have crated from our gitlab would have been lost if we hadn't saved it in registry ( DOCKER HUB is a public registry ). 
Gitlab and AWS offer private registry.

Go to Packages & Registries >> Container Registry. It contains the instructions for logging in into docker registry provided by github, instructions on 
building the package and then pushing the package to docker registry. Later, when the CI/CD job completes this section shows the docker packages that are build.

We are using services to run the docker container with image of "curlimages/curl". 

Set up new environment for elastic beanstalk and configure the s3 bucket name for it.

Create Deploy token as - 
Goto settings >> Repository >> Deploy Token ( Expand ) >> Name, UserName -> AWS >> Scopes (read_repository, read_registry ) >> Create Deploy token 
You will get a deploy token now  - v1iMAznngPy8JNZ_aovJ 
Add a new variable in GitLab with the following key-value pair: {GITLAB_DEPLOY_TOKEN, AWS:v1iMAznngPy8JNZ_aovJ}

Till this point, we have created Docker image which is present in Gitlab registry, now to deploy this image in AWS BeanStalk we need to provide credentials ( Deploy token  ) 
to AWS. 

Dockerrun.aws.json

{
  "AWSEBDockerrunVersion": "1",
  "Image": {
    "Name": "$CI_REGISTRY_IMAGE:$APP_VERSION"                           # Docker Image specified here
  },
  "Authentication": {
    "Bucket": "$AWS_S3_BUCKET",                                         # s3 bucket created by BeanStalk is defined here 
    "Key": "auth.json"
  },
  "Ports": [
    {
      "ContainerPort": 80
    }
  ]
}

Dockerun.aws.public.json

{
  "AWSEBDockerrunVersion": "1",
  "Image": {
    "Name": "nginx"
  },
  "Ports": [
    {
      "ContainerPort": 80
    }
  ]
}

auth.json

{
    "auths": {
        "$CI_REGISTRY": {
            "auth": "$DEPLOY_TOKEN"                                     # Deploy token specified here 
        }
    }
}


A new Job is added as follows: 

deploy to production:
    image:
        name: amazon/aws-cli:2.4.11
        entrypoint: [""]
    stage: deploy
    environment: beanstalk                                                  # s3 bucket created by beanstalk is under beanstalk environment
    script:
        - aws --version
        - yum install -y gettext                                             # gettext utility is needed to use envsubst command ( isn't present by default on aws image )
        - export DEPLOY_TOKEN = $(echo $GITLAB_DEPLOY_TOKEN | td -d "\n" | base64)  # need to convert DEPLOY_TOKEN to base64 as it's expected by aws for authentication 
        - envsubst < templates/Dockerrun.aws.json > Dockerrun.aws.json              # Registry Image and Bucket name are substituted and file Dockerrun.aws.json is copied to present folder
        - envsubst < templates/auth.json > auth.json                                # base64 encoded DEPLOY_TOKEN is substitued and file auth.json is copied to present folder
        - cat Dockerrun.aws.json
        - cat auth.json
        - aws s3 cp Dockerrun.aws.json s3://$AWS_S3_BUCKET/Dockerrun.aws.json       # Dockerrun.aws.json file is uploaded to s3 bucket 
        - aws s3 cp auth.json s3://$AWS_S3_BUCKET/auth.json                         # auth.json file is uploaded to s3 bucket
        # Dockerfile file is used for "docker build"   
        - aws elasticbeanstalk create-application-version --application-name $APP_NAME --version-label $APP_VERSION --source-bundle S3Bucket=$AWS_S3_BUCKET,S3Key=Dockerrun.aws.json
        # running the application in beanstalk application
        - aws elasticbeanstalk update-environment --application-name $APP_NAME --version-label $APP_VERSION --environment-name $APP_ENV_NAME

Now, you can see your website running in AWS_Beanstalk. 
To test whether the application is running properly or not, you can use the following commands - 

        - aws elasticbeanstalk wait environment-updated --application-name $APP_NAME --version-label $APP_VERSION --environment-name $APP_ENV_NAME  # Instead of sleep 10, we are running curl command only after necessary updates have been made in AWS
        - curl $CI_ENVIRONMENT_URL/version.html | grep $APP_VERSION                 # using curl to check that we have desired version of Application

In AWS, on IAM the user gitlab has been given Full Access to AWS Beanstalk including previous access to S3 buckets.

-- Close all the AWS services -- 


Useful Links realted to the Notes:
https://www.youtube.com/watch?v=PGyhBwLyK2U
https://gitlab.com/ajdahal1996
https://docs.gitlab.com/ee/ci/variables/predefined_variables.html
https://us-east-1.console.aws.amazon.com/elasticbeanstalk/home?region=us-east-1#/environment/dashboard?applicationName=my-website&environmentId=e-sfgmqqzjfn